{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>How To Retrieve Unstructred Web Data In a Structured Manner with Riko</center>\n",
    "## <center>A Riveting 15-688 Tutorial</center><br/><center>*by* Ahmet Emre Unal ([aemreunal](https://github.com/aemreunal))</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have heard about [Google Reader](https://en.wikipedia.org/wiki/Google_Reader). It was a free [RSS](https://en.wikipedia.org/wiki/RSS) reader that brought RSS reading to the masses. It was a great product and I, personally, was a very heavy user. Google Reader allowed me to follow many websites that publish things infrequently. This, though, was only possible through the RSS feeds published by the websites.\n",
    "\n",
    "It's great when a website admin takes the time to create the necessary RSS feeds (or implement the tool that does it) but every so often, you come across websites that you want to follow but don't have an RSS feed. How can you now make use of this beautiful system? Can you somehow parse the plain HTML web page to retrieve data in an ordered fashion?\n",
    "\n",
    "[The Riko library](https://github.com/nerevu/riko/) is a library that allows you to do exactly that. By using Riko, we can parse the plain HTML of a website and retrieve the elements in a website in an orderly fashion, like iterating through ```<li>``` elements with a for-loop, for example. \n",
    "\n",
    "I personally believe in walking through examples to learn something so let's jump right in (If you would like to follow along, you can [install Riko](https://github.com/nerevu/riko/blob/master/README.rst#installation) on your local environment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from riko.collections.sync import SyncPipe\n",
    "\n",
    "def get_test_site_url(test_site_name):\n",
    "    return 'file://' + os.getcwd() + '/test_sites/' + test_site_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "#\n",
    "# Note: You can use the following section to create the test sites' files:\n",
    "#\n",
    "##########################################################################################\n",
    "\n",
    "test_site_1_contents = '''<!DOCTYPE html>\\n<html>\\n<body>\\n\\n<h4>This is a simple example</h4>\n",
    "<div class=\"container\">\\n    <ul>\\n      <li class=\"drink hot\">Coffee</li>\n",
    "      <li class=\"drink hot\">Green Tea</li>\\n      <li class=\"hot drink\">Black Tea</li>\n",
    "      <li class=\"drink cold\">Milk</li>\\n      <li class=\"food\">Chocolate</li>\n",
    "      <li class=\"food\">Marshmallow</li>\n",
    "    </ul>\\n</div>\\n\\n</body>\\n</html>\\n'''\n",
    "\n",
    "test_site_2_contents = '''<!DOCTYPE html>\\n<html>\\n<body>\\n\\n<h4>This is a slightly more complex example</h4>\n",
    "<div class=\"container\">\\n    <ul>\\n      <li class=\"drink hot\">Coffee</li>\n",
    "      <li class=\"drink hot\">Green Tea\\n          <p>Oolong Tea</p>\n",
    "          <a href=\"https://en.wikipedia.org/wiki/Oolong\"></a>\\n      </li>\\n      <li class=\"hot drink\">Black Tea\n",
    "          <p>Rize Tea</p>\\n          <a href=\"https://en.wikipedia.org/wiki/Rize_Tea\"></a>\\n      </li>\n",
    "      <li class=\"drink cold\">Milk</li>\\n      <li class=\"food\">Chocolate</li>\n",
    "      <li class=\"food\">Marshmallow</li>\\n    </ul>\\n</div>\\n\\n</body>\\n</html>\\n'''\n",
    "\n",
    "# You can use the following functions to create the test sites' files:\n",
    "\n",
    "path = os.getcwd() + '/test_sites/'\n",
    "\n",
    "# Check if 'test_sites' folder exists\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)  # Create the 'test_sites' folder\n",
    "    \n",
    "# Check if 'test1.html' file exists\n",
    "if not os.path.exists(path + 'test1.html'):\n",
    "    with open(path + 'test1.html', \"w\") as test_site_1:\n",
    "        test_site_1.write(test_site_1_contents)\n",
    "    \n",
    "# Check if 'test2.html' file exists\n",
    "if not os.path.exists(path + 'test2.html'):\n",
    "    with open(path + 'test2.html', \"w\") as test_site_2:\n",
    "        test_site_2.write(test_site_2_contents)\n",
    "\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ```test_sites``` folder, you will find some number of HTML files that are simple website examples. The first one, ```test1.html```, is as follows:\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "\n",
    "<h4>This is a simple example</h4>\n",
    "<div class=\"container\">\n",
    "    <ul>\n",
    "      <li class=\"drink hot\">Coffee</li>\n",
    "      <li class=\"drink hot\">Green Tea</li>\n",
    "      <li class=\"hot drink\">Black Tea</li>\n",
    "      <li class=\"drink cold\">Milk</li>\n",
    "      <li class=\"food\">Chocolate</li>\n",
    "      <li class=\"food\">Marshmallow</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "Riko sees things through what's called a 'pipe'. By fetching a webpage through a URL and pointing Riko to the appropriate part of said webpage, we can obtain 'streams' coming from those 'pipe's that can be iterated. Let's start with a very simple act of retrieveing the webpage in its entirety. We can achieve this with the very simple [```fetchpage```](https://github.com/nerevu/riko/blob/master/riko/modules/fetchpage.py) module, which will literally just fetch a page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = get_test_site_url('test1.html')          # The URL of our test website\n",
    "fetch_conf = {'url': url}                      # A configuration dictionary for Riko\n",
    "pipe = SyncPipe('fetchpage', conf=fetch_conf)  # A pipe that streams 'test1.html'\n",
    "stream = pipe.output                           # The stream being output from the pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did was to tell Riko to create a synchronous pipe (using the [SyncPipe class](https://github.com/nerevu/riko/blob/master/riko/collections/sync.py)) that uses the webpage fetching module (called ```fetchpage```) to fetch the URL specified in the ```fetch_conf``` configuration dictionary. \n",
    "\n",
    "We could've created the stream driectly by simply using the ```fetchpage``` module directly:\n",
    "```python\n",
    "from riko.modules import fetchpage\n",
    "stream = fetchpage.pipe(conf=fetch_conf)\n",
    "```\n",
    "but we'll see in a bit why we're using the ```SyncPipe``` class.\n",
    "\n",
    "You might've wondered when did Riko even have the time to go fetch the page? Well, pipes in Riko are *lazy*. That means it won't start fetching (or processing) a URL before we start iterating. So let's iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in stream:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I told you it would literally just fetch the entire page:\n",
    "```python\n",
    "{u'content': '<!DOCTYPE html>\\n\\n<html>\\n\\n<body>\\n\\n\\n\\n<h4>This is a simple example</h4>\\n\\n<div class=\"container\">\\n\\n    <ul>\\n\\n      <li class=\"drink hot\">Coffee</li>\\n\\n      <li class=\"drink hot\">Green Tea</li>\\n\\n      <li class=\"hot drink\">Black Tea</li>\\n\\n      <li class=\"drink cold\">Milk</li>\\n\\n      <li class=\"food\">Chocolate</li>\\n\\n      <li class=\"food\">Marshmallow</li>\\n\\n    </ul>\\n\\n</div>\\n\\n\\n\\n</body>\\n\\n</html>\\n\\n\\n'}\n",
    "```\n",
    "The whole webpage being printed is not really that useful; there is nothing special about this. We could've at least specified a start and end tag for Riko to fetch only that part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fetch_conf = {   # The same config as above, but with the start and end tags to fetch specified\n",
    "    'url': url,\n",
    "    'start': '<body>',\n",
    "    'end': '</body>'\n",
    "}\n",
    "pipe = SyncPipe('fetchpage', conf=fetch_conf)  # A pipe that streams 'test1.html' according to the config above\n",
    "stream = pipe.output                           # The stream being output from the pipe\n",
    "\n",
    "for item in stream:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't very useful either, honestly:\n",
    "```python\n",
    "{u'content': '\\n\\n\\n\\n<h4>This is a simple example</h4>\\n\\n<div class=\"container\">\\n\\n    <ul>\\n\\n      <li class=\"drink hot\">Coffee</li>\\n\\n      <li class=\"drink hot\">Green Tea</li>\\n\\n      <li class=\"hot drink\">Black Tea</li>\\n\\n      <li class=\"drink cold\">Milk</li>\\n\\n      <li class=\"food\">Chocolate</li>\\n\\n      <li class=\"food\">Marshmallow</li>\\n\\n    </ul>\\n\\n</div>\\n\\n\\n\\n'}\n",
    "```\n",
    "To get to the list items we want, we'd need to do some weird string processing. We don't want to do that and that's why we have Riko!\n",
    "***\n",
    "Let's take a side step and ask ourselves a question: a URL is a string that points to a webpage (or a file in the filesystem), but what could point to an element *inside* a webpage? The answer is [XPath](https://en.wikipedia.org/wiki/XPath). 'XPath' is very similar to a URL, only that it denotes a path inside a markup file. For example, the XPath of the ```<ul>``` element in the website structure above is ```/html/body/div/ul```. In turn, each ```<li>``` element under that ```<ul>``` element could be pointed to using the XPath ```/html/body/div/ul/li[<index>]```, where ```<index>``` is the 1-based index (index = 1 is the first element) or all ```<li>``` elements with the XPath ```/html/body/div/ul/li```.\n",
    "***\n",
    "Riko has an alternate module called [```xpathfetchpage```](https://github.com/nerevu/riko/blob/master/riko/modules/xpathfetchpage.py) that can take a URL, as well as an XPath, and can pipe the element pointed by that XPath:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xpath = '/html/body/div/ul'                         # The XPath of the <ul> element\n",
    "xpath_conf = {'xpath': xpath, 'url': url}           # The XPath configuration dictionary for Riko\n",
    "pipe = SyncPipe('xpathfetchpage', conf=xpath_conf)  # A pipe that streams what's pointed by the \n",
    "                                                    # XPath inside 'test1.html'\n",
    "stream = pipe.output                                # The stream being output from the pipe\n",
    "    \n",
    "for item in stream:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, now this seems interesting:\n",
    "```python\n",
    "{u'{http://www.w3.org/1999/xhtml}li': [{u'content': u'Coffee', u'class': u'drink hot'}, {u'content': u'Green Tea', u'class': u'drink hot'}, {u'content': u'Black Tea', u'class': u'hot drink'}, {u'content': u'Milk', u'class': u'drink cold'}, {u'content': u'Chocolate', u'class': u'food'}, {u'content': u'Marshmallow', u'class': u'food'}]}\n",
    "```\n",
    "The pipe seems to have retrieved a dictionary with a single key, ```u'{http://www.w3.org/1999/xhtml}li'``` (weird key, I know), which points to a list of dictionaries, like ```{u'content': u'Coffee', u'class': u'drink hot'}```, that look eerily similar to our list elements! But it's still tedious at this point to unwrap that outer dictionary. Let's try pointing Riko to an XPath that matches all multiple `<li>` elements, which is ```/html/body/div/ul/li```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xpath = '/html/body/div/ul/li'                      # The XPath of the <li> element(s)\n",
    "xpath_conf = {'xpath': xpath, 'url': url}           # The XPath configuration dictionary for Riko\n",
    "pipe = SyncPipe('xpathfetchpage', conf=xpath_conf)  # A pipe that streams what's pointed by the \n",
    "                                                    # XPath inside 'test1.html'\n",
    "stream = pipe.output                                # The stream being output from the pipe\n",
    "    \n",
    "for item in stream:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're talking:\n",
    "```python\n",
    "{u'content': u'Coffee', u'class': u'drink hot'}\n",
    "{u'content': u'Green Tea', u'class': u'drink hot'}\n",
    "{u'content': u'Black Tea', u'class': u'hot drink'}\n",
    "{u'content': u'Milk', u'class': u'drink cold'}\n",
    "{u'content': u'Chocolate', u'class': u'food'}\n",
    "{u'content': u'Marshmallow', u'class': u'food'}\n",
    "```\n",
    "We have retrieved each ```<li>``` element as a seperate item through the stream we created.\n",
    "\n",
    "As mentioned above, we could've retrieved a specific ```<li>``` element by specifying its index on the XPath; adding '`[1]`' to the end of the XPath above will return:\n",
    "```python\n",
    "{u'content': u'Coffee', u'class': u'drink hot'}\n",
    "```\n",
    "\n",
    "Let's say we are only interested in the drinks. How do we only get the drinks? Do we check for and do some weird string matching with the ```class``` of each element *while* iterating over the stream elements and only add ones that match our criteria? Nope!\n",
    "\n",
    "The point of having streams and pipes is to *filter* the streams and prevent the unwanted objects from going through the stream in the first place. Riko has a way to filter streams, by using the very handy [```filter```](https://github.com/nerevu/riko/blob/master/riko/modules/filter.py) pipe module. The gist of thinking in Riko's terms is to think of chaining pipes together. The first pipe will be carrying a flow of ```<li>``` elements we pointed to. The second pipe, the ```filter``` pipe, will only let through elements that match a certain criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = get_test_site_url('test1.html')               # The URL of our test website\n",
    "xpath = '/html/body/div/ul/li'                      # The XPath of the <li> element(s)\n",
    "xpath_conf = {'xpath': xpath, 'url': url}           # The XPath configuration dictionary for Riko\n",
    "pipe = SyncPipe('xpathfetchpage', conf=xpath_conf)  # A pipe that streams what's pointed by the \n",
    "                                                    # XPath inside 'test1.html'\n",
    "filter_rule = {                                     # A 'filter' rule that tells the 'filter'\n",
    "    'field': 'class',                               # pipe to perform the 'contains' operation on the 'class'\n",
    "    'op': 'contains',                               # field, to check wether the value 'drink' exists, and\n",
    "    'value': 'drink'                                # only let through the items that do match the rule\n",
    "}\n",
    "filter_conf = {'rule': filter_rule}                 # The 'filter' pipe configuration created from the rule\n",
    "pipe = pipe.filter(conf=filter_conf)                # A chained pipe that filters acording to the configuration\n",
    "stream = pipe.output                                # The stream being output from the pipe\n",
    "    \n",
    "for item in stream:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is getting really cool:\n",
    "```python\n",
    "{u'content': u'Coffee', u'class': u'drink hot'}\n",
    "{u'content': u'Green Tea', u'class': u'drink hot'}\n",
    "{u'content': u'Black Tea', u'class': u'hot drink'}\n",
    "{u'content': u'Milk', u'class': u'drink cold'}\n",
    "```\n",
    "We seemed to have retrieved all the drinks, and only the drinks! A similar operation can be performed to only retrieve the hot drinks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = get_test_site_url('test1.html')               # The URL of our test website\n",
    "xpath = '/html/body/div/ul/li'                      # The XPath of the <li> element(s)\n",
    "xpath_conf = {'xpath': xpath, 'url': url}           # The XPath configuration dictionary for Riko\n",
    "pipe = SyncPipe('xpathfetchpage', conf=xpath_conf)  # A pipe that streams what's pointed by the \n",
    "                                                    # XPath inside 'test1.html'\n",
    "filter_rule = {                                     # A 'filter' rule that tells the 'filter'\n",
    "    'field': 'class',                               # pipe to perform the 'contains' operation on the 'class'\n",
    "    'op': 'contains',                               # field, to check whether the value 'drink hot' exists, and\n",
    "    'value': 'drink hot'                            # only let through the items that do match the rule\n",
    "}\n",
    "filter_conf = {'rule': filter_rule}                 # The 'filter' pipe configuration created from the rule\n",
    "pipe = pipe.filter(conf=filter_conf)                # A chained pipe that filters acording to the configuration\n",
    "stream = pipe.output                                # The stream being output from the pipe\n",
    "    \n",
    "for item in stream:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, this even cooler:\n",
    "```python\n",
    "{u'content': u'Coffee', u'class': u'drink hot'}\n",
    "{u'content': u'Green Tea', u'class': u'drink hot'}\n",
    "```\n",
    "but it seems like we have a problem: the fact that the '```value```' key in the rule above has a '```drink hot```' value means that it's not matching an ```<li>``` element with the class '```hot drink```', which is perfectly valid and equal to the class '```drink hot```'. It looks like having a long, more specific value can get pretty unwieldy. It seems to me like it would make more sense if we could apply shorter, more general, *multiple* rules to the ```filter``` pipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = get_test_site_url('test1.html')               # The URL of our test website\n",
    "xpath = '/html/body/div/ul/li'                      # The XPath of the <li> element(s)\n",
    "xpath_conf = {'xpath': xpath, 'url': url}           # The XPath configuration dictionary for Riko\n",
    "pipe = SyncPipe('xpathfetchpage', conf=xpath_conf)  # A pipe that streams what's pointed by the \n",
    "                                                    # XPath inside 'test1.html'\n",
    "filter_rule_drink = {                               # A 'filter' rule that tells the 'filter'\n",
    "    'field': 'class',                               # pipe to perform the 'contains' operation on the 'class'\n",
    "    'op': 'contains',                               # field, to check whether the value 'drink' exists, and\n",
    "    'value': 'drink'                                # only let through the items that do match the rule\n",
    "}\n",
    "filter_rule_hot = {                                 # A 'filter' rule that tells the 'filter'\n",
    "    'field': 'class',                               # pipe to perform the 'contains' operation on the 'class'\n",
    "    'op': 'contains',                               # field, to check whether the value 'hot' exists, and\n",
    "    'value': 'hot'                                  # only let through the items that do match the rule\n",
    "}\n",
    "filter_conf = {                                     # The 'filter' pipe configuration created from the two\n",
    "    'rule': [filter_rule_drink, filter_rule_hot]    # rules specified above\n",
    "}\n",
    "pipe = pipe.filter(conf=filter_conf)                # A chained pipe that filters acording to the configuration\n",
    "stream = pipe.output                                # The stream being output from the pipe\n",
    "    \n",
    "for item in stream:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you heard? They're saying you're the coolest kid on the block:\n",
    "```python\n",
    "{u'content': u'Coffee', u'class': u'drink hot'}\n",
    "{u'content': u'Green Tea', u'class': u'drink hot'}\n",
    "{u'content': u'Black Tea', u'class': u'drink hot'}\n",
    "```\n",
    "It seems to be pretty clear how you can apply different filters to get the elements you want. You can use the filter pipe to filter based on the content as well, to, for example, print only the teas:\n",
    "```python\n",
    "filter_rule = {          # A 'filter' rule that tells the 'filter'\n",
    "    'field': 'content',  # pipe to perform the 'contains' operation on the 'content'\n",
    "    'op': 'contains',    # field, to check whether the value 'tea' exists, and\n",
    "    'value': 'tea'       # only let through the items that do match the rule\n",
    "}\n",
    "```\n",
    "which, when used in the ways above, would print:\n",
    "```python\n",
    "{u'content': u'Green Tea', u'class': u'drink hot'}\n",
    "{u'content': u'Black Tea', u'class': u'hot drink'}\n",
    "```\n",
    "You can notice that the rule was applied case-insensitively.\n",
    "***\n",
    "Through all of these streams, you can use the items, which are plain old Python objects, in any way you want. You can go ahead and print the list of hot drinks you have with the following for-loop:\n",
    "```python\n",
    "for item in stream:\n",
    "    print item['content']  # 'item' object is a regular Python dictionary\n",
    "```\n",
    "which would print:\n",
    "```\n",
    "Coffee\n",
    "Green Tea\n",
    "Black Tea\n",
    "```\n",
    "***\n",
    "Let's look at the following, more complicated webpage structure, which is `test2.html`:\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "\n",
    "<h4>This is a slightly more complex example</h4>\n",
    "<div class=\"container\">\n",
    "    <ul>\n",
    "      <li class=\"drink hot\">Coffee</li>\n",
    "      <li class=\"drink hot\">Green Tea\n",
    "          <p>Oolong Tea</p>\n",
    "          <a href=\"https://en.wikipedia.org/wiki/Oolong\"></a>\n",
    "      </li>\n",
    "      <li class=\"hot drink\">Black Tea\n",
    "          <p>Rize Tea</p>\n",
    "          <a href=\"https://en.wikipedia.org/wiki/Rize_Tea\"></a>\n",
    "      </li>\n",
    "      <li class=\"drink cold\">Milk</li>\n",
    "      <li class=\"food\">Chocolate</li>\n",
    "      <li class=\"food\">Marshmallow</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "How would we access the URLs nested under the teas in the list? If you thought of 'XPath', you can congratulate yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = get_test_site_url('test2.html')               # The URL of our test website\n",
    "xpath = '/html/body/div/ul/li/a'                    # The XPath of the <a> element(s)\n",
    "xpath_conf = {'xpath': xpath, 'url': url}           # The XPath configuration dictionary for Riko\n",
    "pipe = SyncPipe('xpathfetchpage', conf=xpath_conf)  # A pipe that streams what's pointed by the \n",
    "                                                    # XPath inside 'test2.html'\n",
    "stream = pipe.output                                # The stream being output from the pipe\n",
    "    \n",
    "for item in stream:\n",
    "    print item['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like we got both of the URLs:\n",
    "```\n",
    "https://en.wikipedia.org/wiki/Oolong\n",
    "https://en.wikipedia.org/wiki/Rize_Tea\n",
    "```\n",
    "Notice how Riko didn't raise an error for ```<li>``` tags that lack ```<a>``` tags underneath them. This is because the XPath only matches those that do have the ```<a>``` tags. This is very handy for unstructured web data, where some tags might have nested elements, while some might not.\n",
    "***\n",
    "Finally, let's apply what we learned to a real world example: a prominent Turkish writer by the name 'Yılmaz Özdil' publishes an article every day on the newspaper 'Sözcü', talking about the current affairs of Turkey. The newspaper lists his articles under the URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.sozcu.com.tr/kategori/yazarlar/yilmaz-ozdil/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this page, you can see a list of article titles (that link to the articles themselves), along with the date it was published. The XPath of the list elements are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xpath = '/html/body/div[5]/div[6]/div[3]/div[1]/div[2]/div[1]/div[1]/div[2]/ul/li/a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's go ahead and set up a pipe to fetch these list entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xpath_conf = {'xpath': xpath, 'url': url}           # The XPath configuration dictionary for Riko\n",
    "pipe = SyncPipe('xpathfetchpage', conf=xpath_conf)  # A pipe that streams what's pointed by the \n",
    "                                                    # XPath inside the web page\n",
    "stream = pipe.output                                # The stream being output from the pipe\n",
    "    \n",
    "for item in itertools.islice(stream, 3):            # itertools.islice will allow us to get only\n",
    "    print item                                      # the first n elements, which is 3 in this case\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It seems like we retrieved the first 3 articles (the exact articles you retrieve will be different when ran on a different day):\n",
    "```python\n",
    "{u'href': u'http://www.sozcu.com.tr/2016/yazarlar/yilmaz-ozdil/ilelebet-payidar-2-1477851/', u'{http://www.w3.org/1999/xhtml}p': u'\\u0130lelebet payidar', u'{http://www.w3.org/1999/xhtml}span': {u'content': u'30 Ekim 2016', u'class': u'date'}, u'title': u'\\u0130lelebet payidar'}\n",
    "\n",
    "{u'href': u'http://www.sozcu.com.tr/2016/yazarlar/yilmaz-ozdil/cumhuriyet-mucizedir-1475895/', u'{http://www.w3.org/1999/xhtml}p': u'Cumhuriyet, mucizedir', u'{http://www.w3.org/1999/xhtml}span': {u'content': u'29 Ekim 2016', u'class': u'date'}, u'title': u'Cumhuriyet, mucizedir'}\n",
    "\n",
    "{u'href': u'http://www.sozcu.com.tr/2016/yazarlar/yilmaz-ozdil/yarin-bayram-1473877/', u'{http://www.w3.org/1999/xhtml}p': u'Yar\\u0131n bayram...', u'{http://www.w3.org/1999/xhtml}span': {u'content': u'28 Ekim 2016', u'class': u'date'}, u'title': u'Yar\\u0131n bayram...'}\n",
    "```\n",
    "You can notice that each element has a title, a URL and a date. Let's say that we want to parse all of this and return it as a list of tuples, where each entry is of form: `(title, date, url)`. We can do this the old fashioned way, where we iterate through each of those dictionaries and get the data we want. Instead, let's do something a bit different: let's set up two pipes for two different XPaths, and iterate through them synchronously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Top-level <a> elements stream\n",
    "xpath_conf_top = {'xpath': xpath, 'url': url}                   # The XPath config. for the top-level <a> elements\n",
    "pipe_top = SyncPipe('xpathfetchpage', conf=xpath_conf_top)      # A pipe that streams the top-level <a> elements \n",
    "stream_top = pipe_top.output                                    # The stream being output from the pipe\n",
    "  \n",
    "# The child <span> element stream\n",
    "xpath_date = xpath + '/span'                                    # XPath of the <span> children\n",
    "xpath_conf_date = {'xpath': xpath_date, 'url': url}             # The XPath config. for the top-level <a> elements\n",
    "pipe_date = SyncPipe('xpathfetchpage', conf=xpath_conf_date)    # A pipe that streams the top-level <a> elements \n",
    "stream_date = pipe_date.output                                  # The stream being output from the pipe\n",
    "\n",
    "sync_iterator = zip(stream_top, stream_date)                    # Create a synchronous iterator from the two pipes\n",
    "for top_item, date_item in itertools.islice(sync_iterator, 3):  # itertools.islice will allow us to get only\n",
    "    article = (top_item['title'],                               # the first n elements, which is 3 in this case\n",
    "               date_item['content'], \n",
    "               top_item['href'])\n",
    "    print article\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is awesome!\n",
    "```python\n",
    "(u'\\u0130lelebet payidar', u'30 Ekim 2016', u'http://www.sozcu.com.tr/2016/yazarlar/yilmaz-ozdil/ilelebet-payidar-2-1477851/')\n",
    "\n",
    "(u'Cumhuriyet, mucizedir', u'29 Ekim 2016', u'http://www.sozcu.com.tr/2016/yazarlar/yilmaz-ozdil/cumhuriyet-mucizedir-1475895/')\n",
    "\n",
    "(u'Yar\\u0131n bayram...', u'28 Ekim 2016', u'http://www.sozcu.com.tr/2016/yazarlar/yilmaz-ozdil/yarin-bayram-1473877/')\n",
    "```\n",
    "You can go to the website and see the list elements for yourself. For a website that is mostly auto-generated (disastrously, might I say), this was relatively easy to achieve!\n",
    "\n",
    "Let's look at one last example: let's fetch this list and dynamically fetch the articles it points to and get the full article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Article list elements stream\n",
    "xpath_conf_list = {'xpath': xpath, 'url': url}                 # The XPath configuration for the article list\n",
    "pipe_list = SyncPipe('xpathfetchpage', conf=xpath_conf_list)   # A pipe that streams the article list elements\n",
    "stream_list = pipe_list.output                                 # The stream being output from the pipe\n",
    "\n",
    "# The article stream\n",
    "xpath_article = '/html/body/div[5]/div[6]/div[3]/div/div[2]/div[1]/div/div[2]/div[2]'  # XPath of article body\n",
    "\n",
    "xpath_conf_article = {                                         # The XPath configuration for the articles\n",
    "    'url': {'subkey': 'href'},                                 # Notice how we can refer to a 'subkey' as the\n",
    "    'xpath': xpath_article                                     # URL of this configuration\n",
    "}\n",
    "pipe_article = pipe_list.xpathfetchpage(                       # A pipe that streams the articles linked to\n",
    "    conf=xpath_conf_article                                    # by the list stream\n",
    ")                                                              # Notice how we create this pipe by chaining a\n",
    "                                                               # pipe on top of the list pipe; how this one is\n",
    "                                                               # 'dependent' on the list pipe\n",
    "stream_article = pipe_article.output                           # The stream being output from the article pipe\n",
    "\n",
    "sync_iterator = zip(stream_list, stream_article)               # Create a synchronous iterator from the two pipes\n",
    "for list_item, article in itertools.islice(sync_iterator, 3):  # itertools.islice will allow us to get only\n",
    "                                                               # the first n elements, which is 3 in this case\n",
    "    p_elements = article['{http://www.w3.org/1999/xhtml}p']    # Get the list of <p> elements under this XPath\n",
    "    article_body = [paragraph                                  # Grab only the strings under the <p> elements\n",
    "                    for paragraph in p_elements\n",
    "                    if type(paragraph) in [str, unicode]]\n",
    "    article_body = '\\n'.join(article_body)                     # Join strings to create the whole article\n",
    "    article = (list_item['title'], article_body)               # Create the article's (title, body) tuple\n",
    "    print article\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a script to fetch the articles and read them easily, without needing to go to the website: \n",
    "```python\n",
    "(u'\\u0130lelebet payidar', u\"17 Kas\\u0131m 1938.\\n*\\nMaalesef, izdihamdan dalga ...\")\n",
    "\n",
    "(u'Cumhuriyet, mucizedir', u\"*\\nYanm\\u0131\\u015f bina say\\u0131s\\u0131 115 bin, ...\")\n",
    "\n",
    "(u'Yar\\u0131n bayram...', u\"An\\u0131tkabir'e gitti\\u011finde seni en \\xe7ok etk ...\")\n",
    "```\n",
    "The article body looks nicely formatted when you print it. Can this be the most effective ad blocker?\n",
    "***\n",
    "The power of Riko and its pipes may not be immediately visible through parsing just a website but as you explore different options, you can appreciate the power it gives you, the developer, over the mess that is HTML and the World Wide Web. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
